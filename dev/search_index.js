var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#SLOPE.SLOPE","page":"API Reference","title":"SLOPE.SLOPE","text":"module SLOPE\n\nSorted L-One Penalized Estimation\n\n\n\n\n\n","category":"module"},{"location":"api/#SLOPE.slope","page":"API Reference","title":"SLOPE.slope","text":"slope(x, y; kwargs...) -> NamedTuple\n\nFit a SLOPE (Sorted L1 Penalized Estimation) model to the provided data.\n\nSLOPE is a regularization method that combines the L1 norm with a sorted penalty, encouraging both sparsity and grouping of features.\n\nArguments\n\nx: Matrix of predictors (dense or sparse)\ny: Response variable (vector)\n\nKeyword Arguments\n\nα::Union{AbstractVector,Real,Nothing}=nothing: Alpha sequence for regularization path\nλ::Union{AbstractVector,Nothing}=nothing: Lambda sequence for regularization path\nfit_intercept::Bool=true: Whether to fit an intercept term\nloss::Symbol=:quadratic: Type of loss function\ncentering::Symbol=:mean: Method for centering predictors\nscaling::Symbol=:sd: Method for scaling predictors\npath_length::Int=100: Number of regularization path points\ntol::Float64=1e-5: Convergence tolerance for optimization\nmax_it::Int=10000: Maximum number of iterations\nq::Float64=0.1: Parameter for regularization sequence. Should be in the range (0, 1). \nmax_clusters::Union{Int,Nothing}=nothing: Early path stopping criteria for maximum number of clusters (defaults to n+1) \ndev_change_tol::Float64=1e-5: Early path stopping criteria for tolerance for change in deviance \ndev_ratio_tol::Float64=0.999: Early path stopping criteria for tolerance for ratio of deviance\nα_min_ratio::Union{Float64,Nothing}=nothing: Fraction of maximum α to use as minimum value in the regularization path. Defaults to 1e-2 if n > p * m, otherwise 1e-4.`\n\nReturns\n\nA SlopeFit object.\n\n\n\n\n\n","category":"function"},{"location":"api/#SLOPE.SlopeFit","page":"API Reference","title":"SLOPE.SlopeFit","text":"SlopeFit\n\nA structure containing the results of fitting a SLOPE model.\n\nFields\n\nintercepts::Vector{Vector{Float64}}: A vector of intercept vectors along the regularization path. For each point in the path, contains a vector of length m with class-specific intercepts.\ncoefficients::Vector{SparseMatrixCSC{Float64,Int}}: A vector of sparse coefficient matrices  along the regularization path. Each matrix is of size p×m where p is the number of  predictors and m is the number of response classes (1 for regression).\nα::Vector{Float64}: The alpha values used at each point of the regularization path.\nλ::Vector{Float64}: The lambda values used at each point of the regularization path.\nm::Int: The number of response classes (1 for regression, >1 for multinomial).\nloss::String: The loss function used in the model fitting process.\nclasses::Union{Vector,Nothing}: A vector of unique class labels for the response variable. This is nothing for regression models (continuous responses).\n\n\n\n\n\n","category":"type"},{"location":"api/#SLOPE.slopecv","page":"API Reference","title":"SLOPE.slopecv","text":"slopecv(\n  x,\n  y;\n  α=nothing,\n  λ=nothing,\n  γ=[0.0],\n  q=[0.1],\n  n_folds=10,\n  n_repeats=1,\n  metric=:mse,\n  kwargs...\n)\n\nPerform cross-validation for SLOPE to find optimal hyperparameters.\n\nArguments\n\nx::Union{AbstractMatrix,SparseMatrixCSC}: Input feature matrix, can be dense or sparse.\ny::AbstractVector: Response vector.\n\nKeyword Arguments\n\nα::Union{AbstractVector,Real,Nothing}=nothing: SLOPE regularization path. If nothing, it's automatically generated.\nλ::Union{AbstractVector,Nothing}=nothing: Sequence of regularization parameters. If nothing, it's automatically generated.\nγ::Union{AbstractVector,Real}=[0.0]: Parameter controlling the regularization sequence. Multiple values create a grid search.\nq::Union{AbstractVector}=[0.1]: FDR parameter for BH sequence. Multiple values create a grid search.\nn_folds::Int=10: Number of cross-validation folds.\nn_repeats::Int=1: Number of times to repeat the CV process with different fold assignments.\nmetric::Symbol=:mse: Evaluation metric for cross-validation. Options include \"mse\", \"mae\", \"accuracy\", etc.\nkwargs...: Additional parameters passed to the SLOPE solver.\n\nReturns\n\nA SlopeCvResult object.\n\nExamples\n\n# Basic usage with default parameters\nresult = slope(X, y)\n\n# Cross-validation with custom parameters\nresult = slopecv(X, y, γ=[0.0, 0.1, 0.5], q=[0.1, 0.05], n_folds=5, metric=:accuracy)\n\n# Access best parameters and score\nbest_q = result.best_params[\"q\"]\nbest_γ = result.best_params[\"γ\"]\nbest_score = result.best_score\n\nSee Also\n\nslope: For fitting a SLOPE model with fixed parameters.\n\n\n\n\n\n","category":"function"},{"location":"api/#SLOPE.SlopeGridResult","page":"API Reference","title":"SLOPE.SlopeGridResult","text":"SlopeGridResult\n\nResults for a specific hyperparameter combination in the SLOPE cross-validation grid search.\n\nFields\n\nparams::Dict{String,Any}: Dictionary of hyperparameter values (e.g., \"q\", \"γ\")\nscores::Matrix{Real}: Cross-validation scores for each fold and alpha value\nalphas::Vector{Real}: Sequence of alpha values for the regularization path\nscores_means::Vector{Real}: Mean score across folds for each alpha\nscores_errors::Vector{Real}: Standard errors of scores across folds\n\n\n\n\n\n","category":"type"},{"location":"api/#SLOPE.SlopeCvResult","page":"API Reference","title":"SLOPE.SlopeCvResult","text":"SlopeCvResult\n\nResult structure from SLOPE cross-validation.\n\nFields\n\nmetric::Symbol: The evaluation metric used (e.g., \"mse\", \"accuracy\")\nbest_score::Real: The best score achieved during cross-validation\nbest_ind::Int: Index of the best parameter combination\nbest_α_ind::Int: Index of the best alpha value in the regularization path\nbest_params::Dict{String,Any}: Dictionary with the best parameter values\nresults::Vector{SlopeGridResult}: Grid search results for each parameter combination\n\n\n\n\n\n","category":"type"},{"location":"#SLOPE","page":"Home","title":"SLOPE","text":"","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install the package using the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add SLOPE","category":"page"},{"location":"","page":"Home","title":"Home","text":"Alternatively, you can also install the latest development version of the package from the source code on GitHub by calling","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url = \"https://github.com/jolars/SLOPE.jl\")","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SLOPE is a Julia package for Sorted L1 Penalized Estimation (SLOPE), which is a type of regularized regression. SLOPE minimizes the following objective function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"frac1n sum_i=1^n f(y_i x_i^intercal beta) + alpha sum_j=1^p lambda_j beta_(j)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where f(yeta) is the negative log-likelihood contribution of a single observation (y eta). beta_(j) is the j-th coefficient, beta_0 is the intercept, and lambda is a decreasing sequence of regularization weights. x_i is the ith row of the design matrix, and n is the number of observations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"SLOPE is a type of sparse regression, which means that it will, given high enough penalization, set some of the coefficients to zero, effectively removing the corresponding features from the model. If you are familiar with the lasso, then you should know that SLOPE is actually a generalization of the lasso (which you can see by setting all lambda_j to the same value). Unlike the lasso, however, SLOPE also clusters coefficients by settings them to the same magnitude. This helps remove some deficiencies of the lasso in highly correlated settings, and it also allows for selection of more features that is possible with the lasso.","category":"page"},{"location":"#Basic-Usage","page":"Home","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First, we'll load the package and fit a simple model:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SLOPE\nusing Random\nusing Statistics\n\n# Generate some sample data\nn, p = 100, 20\nRandom.seed!(123)\nX = randn(n, p)\nβ = vcat(fill(3.0, 5), fill(0.0, p-5))  # 5 non-zero coefficients\ny = X * β + 0.5 * randn(n)\n\n# Fit a SLOPE model\nfit = slope(X, y)","category":"page"},{"location":"#Examining-the-Model","page":"Home","title":"Examining the Model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can examine the results of the fitted model:","category":"page"},{"location":"","page":"Home","title":"Home","text":"fit.coefficients","category":"page"},{"location":"#Cross-Validation","page":"Home","title":"Cross-Validation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To determine the optimal regularization strength, you can use cross-validation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Perform cross-validation to find optimal parameters\ncvfit = slopecv(X, y)\n\n# View the optimal α value\ncvfit.best_params","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The SLOPE.jl package is a thin wrapper around the C++ slope library, which provides all of the core functionality. Therefore, if you find any bugs or have feature requests, then it's likely that you should open a ticket in the slope repository rather than here.","category":"page"},{"location":"","page":"Home","title":"Home","text":"That being said, if you find any bugs in the Julia wrapper or there are features in the C++ library that are yet to be implemented in the Julia wrapper, then please open an issue in this repository.","category":"page"}]
}
